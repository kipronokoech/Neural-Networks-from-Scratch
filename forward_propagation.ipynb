{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "miniature-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (3, 1)\n",
      "w1 shape:  (4, 3)\n",
      "w2 shape:  (1, 4)\n",
      "w1 shape:  (4, 3)\n",
      "b1 shape (4, 1)\n",
      "w2 shape:  (1, 4)\n",
      "b2 shape (1, 1)\n",
      "f1 shape (4, 1)\n",
      "z2.shape (1, 1)\n",
      "yhat shape (1, 1)\n",
      "[[0.521]]\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Always round-off numpy output to 3 decimal places.\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "\n",
    "class ForwardPass(object):\n",
    "    \"\"\"\n",
    "    Input - Feature data, target values\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, layers, factor=0.01):\n",
    "        \"\"\"\n",
    "        Our init values - X (the input), y (target values),\n",
    "        learning_rate, factor (determines how small the parameters are)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.layers = layers\n",
    "        self.y = y\n",
    "        self.factor = factor\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        \"\"\"\n",
    "        Argument: value(s) x\n",
    "        Returns: 1 / (1 + e^(-x)) - the sigmoid value\n",
    "        \"\"\" \n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def parameters_initialization(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        parameters -- python dictionary containing initial parameter values:\n",
    "        W1 - weight matrix of shape (n1, n0),\n",
    "        b1 - bias vector of shape (n1, 1),\n",
    "        W2 - weight matrix of shape (n2, n1),\n",
    "        b2 - bias vector of shape (n2, 1), where,\n",
    "        n0 - number of the neurons at the input,\n",
    "        n1 - number of neurons at the hidden layer, and\n",
    "        n2 - number of units at the last/output layer.\n",
    "        \"\"\"    \n",
    "        \n",
    "        # Number of neurons in each layer. We have 3 layers only\n",
    "        n0, n1, n2 = self.layers\n",
    "        \n",
    "        # initialize random seed to ensure that the results are reproducible\n",
    "        np.random.seed(3)\n",
    "        \n",
    "        #Generating parameter values for layer 1\n",
    "        w1 = np.random.randn(n1,n0) * self.factor\n",
    "        b1 = np.zeros((n1,1))\n",
    "        print(\"w1 shape: \", w1.shape)\n",
    "        \n",
    "        #Generating initial parameter values for layer 2\n",
    "        w2 = np.random.randn(n2,n1) * self.factor\n",
    "        b2 = np.zeros((n2,1))\n",
    "        print(\"w2 shape: \", w2.shape)\n",
    "\n",
    "        parameters = {\"w1\": w1,\n",
    "                        \"b1\": b1,\n",
    "                            \"w2\": w2,\n",
    "                              \"b2\": b2}\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    def forward_propagation(self, parameters):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        yhat - model output on one forward pass for all the training examples,\n",
    "        layer_ouputs - a dictionary containing model outputs at each layer.\n",
    "        \"\"\"\n",
    "        # Access parameters from parameters dictionary.\n",
    "        # this parameters are generated from parameters_initialization function.\n",
    "        w1 = parameters[\"w1\"]\n",
    "        print(\"w1 shape: \", w1.shape)\n",
    "        b1 = parameters[\"b1\"]\n",
    "        print(\"b1 shape\", b1.shape)\n",
    "        w2 = parameters[\"w2\"]\n",
    "        print(\"w2 shape: \", w2.shape)\n",
    "        b2 = parameters[\"b2\"]\n",
    "        print(\"b2 shape\", b2.shape)\n",
    "\n",
    "        # Perform computations for each layer\n",
    "        z1 = np.dot(w1, self.X) + b1\n",
    "        f1 = self.sigmoid(z1)\n",
    "        print(\"f1 shape\", f1.shape)\n",
    "        z2 = np.dot(w2, f1) + b2\n",
    "        print(\"z2.shape\", z2.shape)\n",
    "        yhat = self.sigmoid(z2)\n",
    "        print(\"yhat shape\", yhat.shape)\n",
    "\n",
    "        # Just to make sure that the output is of the dimension\n",
    "        # we expect\n",
    "        # It should be a vector of the predictions for the for all examples\n",
    "        # self.X.shape[1] - number of training examples\n",
    "        assert(yhat.shape == (1, self.X.shape[1]))\n",
    "        \n",
    "        layer_outputs = {\"z1\": z1,\n",
    "                             \"f1\": f1,\n",
    "                                 \"z2\": z2,\n",
    "                                     \"yhat\": yhat}\n",
    "            \n",
    "        return yhat, layer_outputs\n",
    "\n",
    "# Input data - Just one data point with 3 features/ columns\n",
    "X = np.array([[7, 8, 10]]).reshape(-1, 1)\n",
    "print(\"Input shape\", X.shape)\n",
    "#Target values\n",
    "y = np.array([1]).reshape(-1, 1)\n",
    "\n",
    "#Defining size of our layers\n",
    "n0 = X.shape[0] #input size = number of features\n",
    "n1 = 4 # 4 neurons on the hidden layer\n",
    "n2 = y.shape[0] # one neuron for output layers\n",
    "\n",
    "#Tuple of our layers.\n",
    "layers = (n0, n1, n2)\n",
    "\n",
    "#Initialize the class for ForwardPass\n",
    "# For a forward propagation we don't actually need the target values\n",
    "s = ForwardPass(X=X, y=y, layers=layers, factor=0.1)\n",
    "\n",
    "# parameters initialization \n",
    "parameters = s.parameters_initialization()\n",
    "\n",
    "#forward propagation implementation\n",
    "y_hat, layers_output = s.forward_propagation(parameters)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-stocks",
   "metadata": {},
   "source": [
    "## 1 training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prompt-riverside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "(3, 1)\n",
      "w1 shape:  (4, 3)\n",
      "w2 shape:  (1, 4)\n",
      "X shape:  (3, 1)\n",
      "w1 shape:  (4, 3)\n",
      "b1 shape (4, 1)\n",
      "w2 shape:  (1, 4)\n",
      "b2 shape (1, 1)\n",
      "f1 shape (4, 1)\n",
      "z2.shape (1, 1)\n",
      "yhat shape (1, 1)\n",
      "[[0.52088767]]\n"
     ]
    }
   ],
   "source": [
    "# Input data \n",
    "X = np.array([[7, 8, 10]]).reshape(-1, 1)\n",
    "print(X.shape)\n",
    "#Target values\n",
    "y = np.array([1]).reshape(-1, 1)\n",
    "\n",
    "#Defining size of our layers\n",
    "n0 = X.shape[0] #input size = number of features\n",
    "print(X.shape)\n",
    "n1 = 4 # 4 neurons on the hidden layer\n",
    "n2 = y.shape[0] # one neuron for output layers\n",
    "\n",
    "#Tuple of our layers.\n",
    "layers = (n0, n1, n2)\n",
    "\n",
    "#Initialize the class for OurNeuralNetwork\n",
    "s = OurNeuralNet(X=X, y=y, layers=layers, learning_rate=0.5)\n",
    "y_hat, _ = s.forward_propagation()\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-grove",
   "metadata": {},
   "source": [
    "## 395 training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "satellite-click",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 shape:  (4, 3)\n",
      "w2 shape:  (1, 4)\n",
      "X shape:  (3, 395)\n",
      "w1 shape:  (4, 3)\n",
      "b1 shape (4, 1)\n",
      "w2 shape:  (1, 4)\n",
      "b2 shape (1, 1)\n",
      "f1 shape (4, 395)\n",
      "z2.shape (1, 395)\n",
      "yhat shape (1, 395)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "# df = pd.read_csv(\"https://kipronokoech.github.io/assets/datasets/marks.csv\")\n",
    "df = pd.read_csv(\"marks.csv\")\n",
    "X = df.drop([\"y\"], axis=1) # feature matrix\n",
    "y = df[\"y\"] # target variable\n",
    "\n",
    "n0 = X.shape[1] #input size = number of features\n",
    "n1 = 4 # 4 neurons on the hidden\n",
    "n2 = 1 # one neuron for output\n",
    "\n",
    "layers = (n0, n1, n2)\n",
    "# note: we need X in the dimension X (#features, #training examples)\n",
    "# therefore we transpose the feature matrix, that is, X.T\n",
    "s = OurNeuralNet(X= X.T, y=y, layers=layers)\n",
    "y_hat, _ = s.forward_propagation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-welcome",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
